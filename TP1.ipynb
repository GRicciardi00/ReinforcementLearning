{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndiOIRgSJOUz"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "#This code generates a virtual display for rendering game images. It remains inactive if your machine is equipped with a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "WWlb_AOMKnfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q gymnasium[classic-control]"
      ],
      "metadata": {
        "id": "fQO-NhYFKoF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides a step-by-step guide for implementing the vanilla Q-learning algorithm. You are required to code the QLearningAgent, following the instructions for each method, and then apply it to various tests outlined below."
      ],
      "metadata": {
        "id": "FDE4HfXlK1_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
        "        \"\"\"\n",
        "Q-Learning Agent based on https://inst.eecs.berkeley.edu/~cs188/sp19/projects.html. You have access to the following instance variables:\n",
        "\n",
        "- `self.epsilon` (exploration probability)\n",
        "- `self.alpha` (learning rate)\n",
        "- `self.discount` (discount rate, also known as gamma)\n",
        "\n",
        "Functions you should utilize:\n",
        "\n",
        "- `self.get_legal_actions(state)` {state, hashable -> list of actions, each is hashable} which returns legal actions for a state.\n",
        "- `self.get_qvalue(state, action)` which returns Q(state, action).\n",
        "- `self.set_qvalue(state, action, value)` which sets Q(state, action) := value.\n",
        "\n",
        "**Important Note:** Please refrain from using `self._qValues` directly; instead, use the dedicated `self.get_qvalue` and `self.set_qvalue` methods.\n",
        "        \"\"\"\n",
        "\n",
        "        self.get_legal_actions = get_legal_actions\n",
        "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.discount = discount\n",
        "\n",
        "    def get_qvalue(self, state, action):\n",
        "        \"\"\" Returns Q(state,action) \"\"\"\n",
        "        return self._qvalues[state][action]\n",
        "\n",
        "    def set_qvalue(self, state, action, value):\n",
        "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
        "        self._qvalues[state][action] = value\n",
        "\n",
        "    #---------------------START OF YOUR CODE---------------------#\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "       Calculate your agent's estimation of V(s) using the current q-values:\n",
        "\n",
        "        \\[ V(s) = \\max_{\\text{over\\_action}} Q(\\text{state, action}) \\]\n",
        "\n",
        "        Note: Consider that q-values may be negative.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        <YOUR CODE>\n",
        "\n",
        "        return value\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"\n",
        "        You should do your Q-Value update here:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
        "        \"\"\"\n",
        "\n",
        "        # agent parameters\n",
        "        gamma = self.discount\n",
        "        learning_rate = self.alpha\n",
        "\n",
        "        <YOUR CODE>\n",
        "\n",
        "        self.set_qvalue(state, action, <YOUR CODE: Q-value> )\n",
        "\n",
        "    def get_best_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the best action to take in a state (using current q-values).\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        <YOUR CODE>\n",
        "\n",
        "        return best_action\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        " Determine the action to take in the current state, incorporating exploration. With a probability of self.epsilon, choose a random action; otherwise, select the best policy action using self.get_best_action.\n",
        "\n",
        "Note: For randomly selecting from a list, use random.choice(list). To generate a True or False value based on a given probability, generate a uniform number in the range [0, 1] and compare it with the specified probability.\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick Action\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        action = None\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # agent parameters:\n",
        "        epsilon = self.epsilon\n",
        "\n",
        "        <YOUR CODE>\n",
        "\n",
        "        return chosen_action"
      ],
      "metadata": {
        "id": "A894anQhKqSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use the Q-Learning agent on the Taxi-v3 environment from OpenAI gym. You will need to complete a few of its functions."
      ],
      "metadata": {
        "id": "JUPgJzC6MBt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"Taxi-v3\", render_mode='rgb_array')\n",
        "\n",
        "n_actions = env.action_space.n"
      ],
      "metadata": {
        "id": "DeiH4ojiMGdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s, _ = env.reset()\n",
        "plt.imshow(env.render())"
      ],
      "metadata": {
        "id": "oZFMUOBQNHX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = QLearningAgent(\n",
        "    alpha=0.5, epsilon=0.25, discount=0.99,\n",
        "    get_legal_actions=lambda s: range(n_actions))"
      ],
      "metadata": {
        "id": "ZpciyWcNNJ6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_and_train(env, agent, t_max=10**4):\n",
        "    \"\"\"\n",
        "This function is designed to:\n",
        "\n",
        "    Execute a complete game, with actions determined by the agent's epsilon-greedy policy.\n",
        "    Train the agent using agent.update(...) whenever applicable.\n",
        "    Return the total reward obtained during the game.\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s, _ = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # get agent to pick action given state s.\n",
        "        a = <YOUR CODE>\n",
        "\n",
        "        next_s, r, done, _, _ = env.step(a)\n",
        "\n",
        "        # train (update) agent for state s\n",
        "        <YOUR CODE>\n",
        "\n",
        "        s = next_s\n",
        "        total_reward += r\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return total_reward"
      ],
      "metadata": {
        "id": "Rk6R1Ym-NM7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "rewards = []\n",
        "for i in range(1000):\n",
        "    rewards.append(play_and_train(env, agent))\n",
        "    agent.epsilon *= 0.99\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(True)\n",
        "        plt.title('eps = {:e}, mean reward = {:.1f}'.format(agent.epsilon, np.mean(rewards[-10:])))\n",
        "        plt.plot(rewards)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "HKd9FiiGNR26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will use a policy. The policy to be employed is the epsilon-greedy policy, wherein the agent selects the optimal action with a probability of (1−ϵ). Otherwise, it randomly samples an action. It's important to note that, by pure chance, the agent may occasionally sample the optimal action even during random selection."
      ],
      "metadata": {
        "id": "wwTX8GKbNxuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SARSA"
      ],
      "metadata": {
        "id": "GTtGSpGAN8RX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}